{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:17:10.121092Z",
     "start_time": "2022-06-02T02:17:10.098888Z"
    }
   },
   "source": [
    "# Fine-tuning distil-BERT for sentiment analysis of product reviews\n",
    "\n",
    "DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts using the BERT base model. You can learn more about this flavor of BERT here: https://huggingface.co/distilbert-base-uncased\n",
    "\n",
    "This notebook showcases the use of pre-trained models in Domino and demonstrates the process of GPU-accelerated fine-tuning using Nvidia GPUs. We use the [Amazon Polarity](https://huggingface.co/datasets/amazon_polarity) dataset, which contains 3.5 million samples of sentiments for product reviews. Due to the size of this dataset, for demonstration purposes we use a [10% sample](https://huggingface.co/datasets/ben-epstein/amazon_polarity_10_pct) of the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:30:37.970878Z",
     "start_time": "2022-06-02T02:30:34.740917Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import nvidia\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    enable_full_determinism,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    BatchEncoding\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "import itertools\n",
    "import mlflow.transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure GPU acceleration is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:44:18.286115Z",
     "start_time": "2022-06-02T02:44:18.222850Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU acceleration is available!\")\n",
    "else:\n",
    "    print(\"GPU acceleration is NOT available! Training, fine-tuning, and inference speed will be adversely impacted.\")\n",
    "    \n",
    "enable_full_determinism(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Let's load the original distilbert dataset and classify a handful of test statments. The NLP pipeline produces a label and a prediction score._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_id, num_labels=2, id2label={0: \"negative\", 1: \"positive\"})\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_id)\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\n",
    "    \"This towel did not match the description. Is it far too small/\",  \n",
    "    \"I love the colors! This is a perfect birthday gift\", \n",
    "    \"It came damanged.\", \n",
    "    \"I'm not sure about it yet. I'll check back in a few weeks and update.\",\n",
    "]\n",
    "results = nlp(sentences)\n",
    "\n",
    "for sample in zip(sentences, results):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Polarity dataset\n",
    "\n",
    "Let's now load the Amazon Polarity dataset. The dataset has two attributes for this task:\n",
    "\n",
    "* **content** - The product review, which we will rename `text`\n",
    "* **label** - sentiment, which we will encode as follows:\n",
    "    * negative  : 0\n",
    "    * positive : 1\n",
    " \n",
    "The huggingface dataset comes with these labels already encoded as `ClassLabel`s, so we shouldn't need to make any changes.\n",
    "    \n",
    "Let's process the dataset and show the first 5 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Domino-ai/amazon_polarity_10_pct\"\n",
    "ds = load_dataset(name)\n",
    "ds = ds.rename_columns({\"content\": \"text\"})\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# Look at the first few rows\n",
    "ds[\"train\"].to_pandas()[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with splitting it into a training, test, and validation sets. As we see, this dataset has a `train` and a `test` split already. So we will split our `train` split into train/val, and save our test split for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training, test, and validation subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the training dataset into a training, test, and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:08.030875Z",
     "start_time": "2022-06-02T02:54:07.999354Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_train_val = ds[\"train\"].train_test_split(test_size=0.1, seed=42, stratify_by_column=\"label\")\n",
    "\n",
    "ds[\"train\"] = ds_train_val[\"train\"]\n",
    "ds[\"validation\"] = ds_train_val[\"test\"]\n",
    "\n",
    "print(f\"Samples in train      : {len(ds['train'])}\")\n",
    "print(f\"Samples in validation : {len(ds['validation'])}\")\n",
    "print(f\"Samples in test       : {len(ds['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's score (make predictions on) the test set using only the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ds[\"test\"][\"text\"][:10]\n",
    "df_test = pd.DataFrame(ds[\"test\"][:10])\n",
    "results = nlp(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a DataFrame with the ground truth and the prediction and see how the pretrained model is doing in terms of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results)\n",
    "df_test[\"label\"] = df_test[\"label\"].replace([0, 1], [\"negative\", \"positive\"])\n",
    "results_df.columns = [\"pred\", \"score\"]\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results_df = pd.concat([df_test[[\"text\", \"label\"]].reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "results_df[\"Correct\"] = results_df[\"label\"].eq(results_df[\"pred\"])\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the accuracy of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = len(results_df[results_df[\"Correct\"] == True]) / len(results_df)\n",
    "\n",
    "print(\"Accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always important to look at your performance per-label, so let's do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.concat([results_df[\"label\"].value_counts(), results_df.groupby(\"label\")[\"Correct\"].mean().mul(100).round(2)], axis=1)\n",
    "accuracy_df = accuracy_df.reset_index()\n",
    "accuracy_df.columns = [\"Label\", \"Count\", \"Accuracy\"]\n",
    "accuracy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Fine-tuning\n",
    "\n",
    "The fine-tuning process takes the base model (distil-bert) and performs additional training, tweaking it towards a more specialized use-case. Here, we'll use the training subset of the Amazon Polarity sentiment dataset. This transfer learning approach will enables us to produce a more accurate model with a smaller training time.\n",
    "\n",
    "### Datasets preparation\n",
    "\n",
    "First, we need to prepare the three datasets (training, validation, and test) by tokenizing their inputs.\n",
    "\n",
    "You'll notice that we don't pad our inputs. This is because it uses a lot of memory, and takes a long time upfront to do so. Instead, in the next step, we pass in our tokenizer during the training process so that our inputs get padded dynamically during training, using less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:54:33.660010Z",
     "start_time": "2022-06-02T02:54:17.948143Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(\n",
    "    tokenizer: DistilBertTokenizer, examples: LazyBatch\n",
    ") -> BatchEncoding:\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=False, max_length=512\n",
    "    )  # 512 because we use BERT\n",
    "\n",
    "\n",
    "ds = ds.map(partial(preprocess_function, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up and training\n",
    "\n",
    "Next, we define the training metrics (in our case, f1) and some additional customization points like training epochs, size of batches etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T02:57:15.963784Z",
     "start_time": "2022-06-02T02:54:33.662575Z"
    }
   },
   "outputs": [],
   "source": [
    "metric_choice = \"f1\"\n",
    "METRIC = evaluate.load(metric_choice)\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction) -> dict:\n",
    "    predictions, labels = np.array(eval_pred.predictions), np.array(eval_pred.label_ids)\n",
    "    predictions = predictions.argmax(axis=1)\n",
    "    return METRIC.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "# Autologging with mlflow directly into Domino\n",
    "mlflow.transformers.autolog(\n",
    "    log_input_examples=True,\n",
    "    log_model_signatures=True,\n",
    "    log_models=False,\n",
    "    log_datasets=False\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir = \"/mnt/artifacts/temp/\",\n",
    "        evaluation_strategy = \"steps\",\n",
    "        learning_rate=0.00001,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=metric_choice,\n",
    "        save_total_limit = 2,\n",
    "        save_strategy = \"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        optim=\"adamw_torch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"validation\"],\n",
    "        compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform the training.\n",
    "\n",
    "**Note that you will need a hardware tier with sufficient memory and compute, ideally a HW tier which provides GPU acceleration. Otherwise the training process can take a substantial amount of time or crash due to not having access to enough system memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can now test the accuracy of the model using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test = trainer.predict(ds[\"test\"]).metrics[\"test_f1\"]\n",
    "print(f\"F1 on test: {f1_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the fine-tuned model\n",
    "\n",
    "Finally, we can save the fine-tuned model and used it for online predictions via a [Model API](https://docs.dominodatalab.com/en/latest/user_guide/8dbc91/host-models-as-rest-apis/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-02T03:09:11.599174Z",
     "start_time": "2022-06-02T03:09:10.847115Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Please change this location accordingly. You might want to change this depending on whether you are using a git based project\n",
    "or a DFS based project and if you want to use this model\n",
    "''' \n",
    "trainer.save_model(\"/mnt/artifacts/amazon-sentiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
